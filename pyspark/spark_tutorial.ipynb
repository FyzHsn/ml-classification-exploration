{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              text|\n",
      "+---+------------------+\n",
      "|  4|       spark i j k|\n",
      "|  5|             l m n|\n",
      "|  6|spark hadoop spark|\n",
      "|  7|     apache hadoop|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "    ], [\"id\", \"text\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "hashing_tf = HashingTF(inputCol='words', outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashing_tf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.2, 0.8], prediction=1.0\n",
      "(5, l m n) --> prob=[0.8, 0.2], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.1, 0.9], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[1.0, 0.0], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on the test document\n",
    "predictions = model.transform(test)\n",
    "results = predictions.select('id', 'text', 'probability', 'prediction')\n",
    "for row in results.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"({}, {}) --> prob={}, prediction={}\".format(rid, text, str([round(prob[0], 1), round(prob[1], 1)]), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+---------+----------+-------+\n",
      "|campaignId|sourceGameId|startCount|viewCount|clickCount|install|\n",
      "+----------+------------+----------+---------+----------+-------+\n",
      "|campaign_1|  candycrush|         5|        4|         3|      1|\n",
      "|campaign_2|  farmvilles|         2|        3|         2|      1|\n",
      "|campaign_1|  candycrush|         0|        2|         4|      0|\n",
      "|campaign_2|  candycrush|         0|        8|         8|      0|\n",
      "|campaign_2|  farmvilles|         6|        3|         4|      1|\n",
      "|campaign_1|  candycrush|         9|        2|         1|      1|\n",
      "+----------+------------+----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.createDataFrame([\n",
    "    ('campaign_1', 'candycrush', 5, 4, 3, 1),\n",
    "    ('campaign_2', 'farmvilles', 2, 3, 2, 1),\n",
    "    ('campaign_1', 'candycrush', 0, 2, 4, 0),\n",
    "    ('campaign_2', 'candycrush', 0, 8, 8, 0),\n",
    "    ('campaign_2', 'farmvilles', 6, 3, 4, 1),\n",
    "    ('campaign_1', 'candycrush', 9, 2, 1, 1)\n",
    "    ], \n",
    "    ['campaignId', 'sourceGameId', 'startCount', \n",
    "    'viewCount', 'clickCount', 'install'])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+---------+----------+-------+\n",
      "|campaignId|sourceGameId|startCount|viewCount|clickCount|install|\n",
      "+----------+------------+----------+---------+----------+-------+\n",
      "|campaign_1|  candycrush|         2|        4|         3|      1|\n",
      "|campaign_2|  farmvilles|         2|        3|         2|      1|\n",
      "|campaign_3|  mangotimes|         2|        3|         2|      1|\n",
      "+----------+------------+----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.createDataFrame([\n",
    "    ('campaign_1', 'candycrush', 2, 4, 3, 1),\n",
    "    ('campaign_2', 'farmvilles', 2, 3, 2, 1),\n",
    "    ('campaign_3', 'mangotimes', 2, 3, 2, 1)\n",
    "    ], \n",
    "    ['campaignId', 'sourceGameId', 'startCount', \n",
    "    'viewCount', 'clickCount', 'install'])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, RobustScaler, VectorAssembler\n",
    "\n",
    "CATEGORICAL_FEATS = [\"campaignId\", \"sourceGameId\"]\n",
    "NUMERICAL_FEATS = [\"startCount\", \"viewCount\", \"clickCount\"]\n",
    "\n",
    "str_indexer = StringIndexer(inputCols=CATEGORICAL_FEATS,\n",
    "                            outputCols=[\"campaignIdIndex\", \"sourceGameIdIndex\"],\n",
    "                            handleInvalid='keep')\n",
    "encoder = OneHotEncoder(inputCols=str_indexer.getOutputCols(),\n",
    "                        outputCols=[\"campaignIdVec\", \"sourceGameIdVec\"],\n",
    "                        handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler1 = VectorAssembler(inputCols=NUMERICAL_FEATS, \n",
    "                             outputCol=\"num_features\")\n",
    "scaler = RobustScaler(inputCol='num_features',\n",
    "                      outputCol='scaled' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler2 = VectorAssembler(inputCols=[\"campaignIdVec\", \"sourceGameIdVec\", \"scaled\"],\n",
    "                             outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"install\", \n",
    "                        maxIter=10, regParam=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|install|\n",
      "+--------------------+-------+\n",
      "|[1.0,0.0,0.0,1.0,...|      1|\n",
      "|[0.0,1.0,0.0,0.0,...|      1|\n",
      "|(9,[0,3,7,8],[1.0...|      0|\n",
      "|(9,[1,3,7,8],[1.0...|      0|\n",
      "|[0.0,1.0,0.0,0.0,...|      1|\n",
      "|[1.0,0.0,0.0,1.0,...|      1|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[str_indexer, encoder, assembler1, scaler, assembler2, lr])\n",
    "pipeline_model = pipeline.fit(training)\n",
    "training = pipeline_model.transform(training)\n",
    "training.select(\"features\", \"install\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[1.0,0.0,0.0,1.0,...|       1.0|\n",
      "|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|[0.0,0.0,1.0,0.0,...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "predictions.select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection Via Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashing_tf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will wrap the pipeline estimator in a cross validator instance.\n",
    "# This allows us to jointly test various parameters over ALL stages of the pipeline.\n",
    "# A CrossValidator requires: (i) an estimator, (ii) a grid of ParamMaps and (iii) and evaluator\n",
    "# ParamGridBuilder can be used to build a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam, we will have a grid \n",
    "# of 2 X 3 to search over.\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(hashing_tf.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.001]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = CrossValidator(estimator=pipeline,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=BinaryClassificationEvaluator(),\n",
    "                           numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cross_val.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.3413, 0.6587]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9438, 0.0562]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3451, 0.6549]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.9561, 0.0439]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cv_model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = TrainValidationSplit(estimator=pipeline,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=BinaryClassificationEvaluator(),\n",
    "                                 trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_val.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|(10,[0,1,6],[1.0,...|       1.0|\n",
      "|(10,[0,5],[2.0,1.0])|       0.0|\n",
      "|(10,[0,6],[1.0,1.0])|       1.0|\n",
      "|(10,[3,5],[1.0,1.0])|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  1|             b d|  0.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = training.sample(fraction=0.8, withReplacement=True)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Data Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf().setAppName('tutorial').setMaster('local[2]')\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6]\n",
    "dist_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4129\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"requirements.txt\")\n",
    "line_length = lines.map(lambda s: len(s))\n",
    "total_length = line_length.reduce(lambda a, b: a + b)\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(s):\n",
    "    words = s.split(\"=\")\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = lines.map(word_count)\n",
    "# word_num.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_libgcc_mutex=0.1=main', 1),\n",
       " ('anaconda=custom=py35_0', 1),\n",
       " ('astroid=2.0.4=py35_0', 1),\n",
       " ('babel=2.2.0=py35_0', 1),\n",
       " ('beautifulsoup4=4.4.1=py35_0', 1),\n",
       " ('chest=0.2.3=py35_0', 1),\n",
       " ('cloudpickle=0.1.1=py35_0', 1),\n",
       " ('clyent=1.2.1=py35_0', 1),\n",
       " ('conda=4.5.11=py35_0', 1),\n",
       " ('conda-build=1.20.0=py35_0', 1),\n",
       " ('cryptography=1.4=py35_0', 1),\n",
       " ('curl=7.45.0=0', 1),\n",
       " ('cycler=0.10.0=py35_0', 1),\n",
       " ('cytoolz=0.7.5=py35_0', 1),\n",
       " ('decorator=4.0.9=py35_0', 1),\n",
       " ('expat=2.2.9=he6710b0_2', 1),\n",
       " ('fastcache=1.0.2=py35_0', 1),\n",
       " ('flask=0.10.1=py35_1', 1),\n",
       " ('flask-cors=2.1.2=py35_0', 1),\n",
       " ('fontconfig=2.13.0=h9420a91_0', 1),\n",
       " ('glib=2.63.1=h5a9c865_0', 1),\n",
       " ('greenlet=0.4.9=py35_0', 1),\n",
       " ('gst-plugins-base=1.14.0=hbbd80ab_1', 1),\n",
       " ('gstreamer=1.14.0=hb453b48_1', 1),\n",
       " ('hdf5=1.8.15.1=2', 1),\n",
       " ('icu=58.2=he6710b0_3', 1),\n",
       " ('idna=2.0=py35_0', 1),\n",
       " ('intel-openmp=2019.4=243', 1),\n",
       " ('ipykernel=4.3.1=py35_0', 1),\n",
       " ('ipython=4.1.2=py35_1', 1),\n",
       " ('ipywidgets=4.1.1=py35_0', 1),\n",
       " ('jbig=2.1=0', 1),\n",
       " ('jdcal=1.2=py35_0', 1),\n",
       " ('jinja2=2.8=py35_0', 1),\n",
       " ('jupyter_client=4.2.2=py35_0', 1),\n",
       " ('kiwisolver=1.0.1=py35hf484d3e_0', 1),\n",
       " ('libdynd=0.7.2=0', 1),\n",
       " ('libgcc-ng=9.1.0=hdf63c60_0', 1),\n",
       " ('libgfortran-ng=7.3.0=hdf63c60_0', 1),\n",
       " ('libpng=1.6.37=hbc83047_0', 1),\n",
       " ('libsodium=1.0.3=0', 1),\n",
       " ('libxcb=1.14=h7b6447c_0', 1),\n",
       " ('libxslt=1.1.28=0', 1),\n",
       " ('llvmlite=0.9.0=py35_0', 1),\n",
       " ('lz4-c=1.9.2=he6710b0_0', 1),\n",
       " ('markupsafe=0.23=py35_0', 1),\n",
       " ('matplotlib=3.0.0=py35h5429711_0', 1),\n",
       " ('mistune=0.7.2=py35_0', 1),\n",
       " ('mkl-rt=11.1=p0', 1),\n",
       " ('mkl-service=1.1.0=py35_p0', 1),\n",
       " ('mpmath=0.19=py35_0', 1),\n",
       " ('multipledispatch=0.4.8=py35_0', 1),\n",
       " ('ncurses=6.0=h9df7e31_2', 1),\n",
       " ('networkx=1.11=py35_0', 1),\n",
       " ('nose=1.3.7=py35_0', 1),\n",
       " ('notebook=4.1.0=py35_1', 1),\n",
       " ('patchelf=0.8=0', 1),\n",
       " ('pep8=1.7.0=py35_0', 1),\n",
       " ('pexpect=4.0.1=py35_0', 1),\n",
       " ('pip=8.1.1=py35_1', 1),\n",
       " ('ply=3.8=py35_0', 1),\n",
       " ('pycparser=2.14=py35_0', 1),\n",
       " ('pycrypto=2.6.1=py35_0', 1),\n",
       " ('pycurl=7.19.5.3=py35_0', 1),\n",
       " ('pygments=2.1.1=py35_0', 1),\n",
       " ('pyparsing=2.0.3=py35_0', 1),\n",
       " ('pyqt=5.9.2=py35h05f1152_2', 1),\n",
       " ('pytest=2.8.5=py35_0', 1),\n",
       " ('python=3.5.4=he2c66cf_20', 1),\n",
       " ('pyyaml=3.11=py35_1', 1),\n",
       " ('readline=7.0=ha6073c6_4', 1),\n",
       " ('ruamel_yaml=0.11.14=py35_1', 1),\n",
       " ('scikit-learn=0.20.0=py35h4989274_1', 1),\n",
       " ('scipy=1.1.0=py35hd20e5f9_0', 1),\n",
       " ('singledispatch=3.4.0.3=py35_0', 1),\n",
       " ('six=1.10.0=py35_0', 1),\n",
       " ('spyder=3.0.2=py35_0', 1),\n",
       " ('statsmodels=0.9.0=py35h3010b51_0', 1),\n",
       " ('terminado=0.5=py35_1', 1),\n",
       " ('toolz=0.7.4=py35_0', 1),\n",
       " ('typed-ast=1.1.0=py35h20cf15d_0', 1),\n",
       " ('werkzeug=0.11.4=py35_0', 1),\n",
       " ('xlwt=1.0.0=py35_0', 1),\n",
       " ('# This file may be used to create an environment using:', 1),\n",
       " ('# $ conda create --name <env> --file <this file>', 1),\n",
       " ('# platform: linux-64', 1),\n",
       " ('alabaster=0.7.7=py35_0', 1),\n",
       " ('anaconda-client=1.4.0=py35_0', 1),\n",
       " ('anaconda-navigator=1.8.4=py35_0', 1),\n",
       " ('anaconda-project=0.8.4=py_0', 1),\n",
       " ('argcomplete=1.0.0=py35_1', 1),\n",
       " ('bitarray=0.8.1=py35_0', 1),\n",
       " ('blas=1.0=mkl', 1),\n",
       " ('boto=2.39.0=py35_0', 1),\n",
       " ('ca-certificates=2020.6.24=0', 1),\n",
       " ('cffi=1.11.5=py35he75722e_1', 1),\n",
       " ('chardet=3.0.4=py35_1', 1),\n",
       " ('colorama=0.3.7=py35_0', 1),\n",
       " ('conda-env=2.6.0=1', 1),\n",
       " ('conda-manager=0.3.1=py35_0', 1),\n",
       " ('configobj=5.0.6=py35_0', 1),\n",
       " ('cython=0.23.4=py35_0', 1),\n",
       " ('dbus=1.13.16=hb2f20db_0', 1),\n",
       " ('dill=0.2.4=py35_0', 1),\n",
       " ('docutils=0.12=py35_0', 1),\n",
       " ('et_xmlfile=1.0.1=py35_0', 1),\n",
       " ('freetype=2.10.2=h5ab3b9f_0', 1),\n",
       " ('gevent=1.1.0=py35_0', 1),\n",
       " ('heapdict=1.0.0=py35_0', 1),\n",
       " ('ipython_genutils=0.1.0=py35_0', 1),\n",
       " ('isort=4.3.4=py35_0', 1),\n",
       " ('itsdangerous=0.24=py35_0', 1),\n",
       " ('jedi=0.9.0=py35_0', 1),\n",
       " ('jpeg=9b=h024ee3a_2', 1),\n",
       " ('jsonschema=2.4.0=py35_0', 1),\n",
       " ('jupyter=1.0.0=py35_2', 1),\n",
       " ('jupyter_console=4.1.1=py35_0', 1),\n",
       " ('jupyter_core=4.1.0=py35_0', 1),\n",
       " ('lazy-object-proxy=1.3.1=py35h14c3975_2', 1),\n",
       " ('libedit=3.1=heed3624_0', 1),\n",
       " ('libffi=3.2.1=hd88cf55_4', 1),\n",
       " ('libgfortran=3.0=0', 1),\n",
       " ('libstdcxx-ng=9.1.0=hdf63c60_0', 1),\n",
       " ('libtiff=4.1.0=h2733197_1', 1),\n",
       " ('libuuid=1.0.3=h1bed415_2', 1),\n",
       " ('libxml2=2.9.10=he19cac6_1', 1),\n",
       " ('locket=0.2.0=py35_0', 1),\n",
       " ('lxml=3.6.0=py35_0', 1),\n",
       " ('mccabe=0.6.1=py35_1', 1),\n",
       " ('mkl=2018.0.3=1', 1),\n",
       " ('nbconvert=4.1.0=py35_0', 1),\n",
       " ('nbformat=4.0.1=py35_0', 1),\n",
       " ('nltk=3.2=py35_0', 1),\n",
       " ('numexpr=2.6.8=py35hd89afb7_0', 1),\n",
       " ('numpy=1.11.3=py35h3dfced4_4', 1),\n",
       " ('olefile=0.46=py_0', 1),\n",
       " ('openpyxl=2.3.2=py35_0', 1),\n",
       " ('openssl=1.0.2u=h7b6447c_0', 1),\n",
       " ('pandas=0.23.4=py35h04863e7_0', 1),\n",
       " ('path.py=8.1.2=py35_1', 1),\n",
       " ('patsy=0.5.0=py35_0', 1),\n",
       " ('pcre=8.44=he6710b0_0', 1),\n",
       " ('pickleshare=0.5=py35_0', 1),\n",
       " ('pillow=5.2.0=py35heded4f4_0', 1),\n",
       " ('psutil=4.1.0=py35_0', 1),\n",
       " ('ptyprocess=0.5=py35_0', 1),\n",
       " ('py=1.4.31=py35_0', 1),\n",
       " ('pyasn1=0.1.9=py35_0', 1),\n",
       " ('pycosat=0.6.3=py35h14c3975_0', 1),\n",
       " ('pyflakes=1.1.0=py35_0', 1),\n",
       " ('pylint=2.1.1=py35_0', 1),\n",
       " ('pyopenssl=16.2.0=py35_0', 1),\n",
       " ('python-dateutil=2.5.1=py35_0', 1),\n",
       " ('pytz=2016.2=py35_0', 1),\n",
       " ('pyzmq=15.2.0=py35_0', 1),\n",
       " ('qt=5.9.6=h52aff34_0', 1),\n",
       " ('qtawesome=0.3.2=py35_0', 1),\n",
       " ('qtconsole=4.7.5=py_0', 1),\n",
       " ('qtpy=1.9.0=py_0', 1),\n",
       " ('redis=2.6.9=0', 1),\n",
       " ('redis-py=2.10.3=py35_0', 1),\n",
       " ('requests=2.12.4=py35_0', 1),\n",
       " ('rope=0.9.4=py35_1', 1),\n",
       " ('seaborn=0.9.0=pyh91ea838_1', 1),\n",
       " ('setuptools=20.3=py35_0', 1),\n",
       " ('simplegeneric=0.8.1=py35_0', 1),\n",
       " ('sip=4.19.8=py35hf484d3e_0', 1),\n",
       " ('snowballstemmer=1.2.1=py35_0', 1),\n",
       " ('sockjs-tornado=1.0.1=py35_0', 1),\n",
       " ('sphinx=1.3.5=py35_0', 1),\n",
       " ('sphinx_rtd_theme=0.1.9=py35_0', 1),\n",
       " ('sqlalchemy=1.0.12=py35_0', 1),\n",
       " ('sqlite=3.23.1=he433501_0', 1),\n",
       " ('sympy=1.0=py35_0', 1),\n",
       " ('tk=8.6.10=hbc83047_0', 1),\n",
       " ('tornado=4.3=py35_0', 1),\n",
       " ('traitlets=4.2.1=py35_0', 1),\n",
       " ('unicodecsv=0.14.1=py35_0', 1),\n",
       " ('util-linux=2.21=0', 1),\n",
       " ('wheel=0.29.0=py35_0', 1),\n",
       " ('wrapt=1.10.11=py35h14c3975_2', 1),\n",
       " ('xlrd=0.9.4=py35_0', 1),\n",
       " ('xlsxwriter=0.8.4=py35_0', 1),\n",
       " ('xz=5.2.5=h7b6447c_0', 1),\n",
       " ('yaml=0.1.6=0', 1),\n",
       " ('zeromq=4.1.3=0', 1),\n",
       " ('zlib=1.2.11=h7b6447c_3', 1),\n",
       " ('zstd=1.4.5=h0b5b093_0', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
