{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, \\\n",
    "                               RobustScaler, \\\n",
    "                               StringIndexer, \\\n",
    "                               VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, \\\n",
    "                              ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, \\\n",
    "                                  col, \\\n",
    "                                  count, \\\n",
    "                                  create_map, \\\n",
    "                                  lit, \\\n",
    "                                  when, \\\n",
    "                                  udf  \n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ads-ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(delimiter=';') \\\n",
    "               .options(header=True) \\\n",
    "               .options(inferSchema=True) \\\n",
    "               .csv('../data/training_data.csv')\n",
    "df.printSchema()\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff_in_minutes(dt_0, dt_1):\n",
    "    if dt_0 is None:\n",
    "        return 0.0\n",
    "    return round((dt_1 - dt_0).total_seconds() / 60.0, 1)    \n",
    "    \n",
    "time_diff_in_min_udf = udf(time_diff_in_minutes, FloatType())\n",
    "preprocessed_df = df.withColumn('timeSinceLastStart', \\\n",
    "                                time_diff_in_min_udf(df.lastStart, df.timestamp)) \\\n",
    "                    .drop(\"id\", \"timestamp\", \"lastStart\")\n",
    "\n",
    "# Count the number of null values per columns\n",
    "preprocessed_df.select([count(when(isnan(c), c)).alias(c) for c in preprocessed_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('install', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count_df = df.groupby('label').agg({'label': 'count'})\n",
    "\n",
    "n_1 = class_count_df.filter(df.label == '1').select(\"count(label)\").collect()[0][0]\n",
    "n_0 = class_count_df.filter(df.label == '0').select(\"count(label)\").collect()[0][0]\n",
    "\n",
    "w_1 = (n_0 + n_1) / (2.0 * n_1)\n",
    "w_0 = (n_0 + n_1) / (2.0 * n_0)\n",
    "\n",
    "class_weights = {0: w_0, 1: w_1}\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*class_weights.items())])\n",
    "df = df.withColumn(\"weights\", mapping_expr.getItem(col(\"label\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is prudent to store the split data in train/test folders for the sake of reproducibility\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2])\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = ['campaignId', 'platform', 'softwareVersion', 'sourceGameId', 'country', \n",
    "                   'connectionType', 'deviceType']\n",
    "numerical_feats = ['startCount', 'viewCount', 'clickCount', 'installCount', 'startCount1d', \n",
    "                   'startCount7d', 'timeSinceLastStart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [] \n",
    "for column in categorical_feats:\n",
    "    str_indexer = StringIndexer(inputCol=column, \n",
    "                                outputCol=column + \"Index\",\n",
    "                                handleInvalid='keep')\n",
    "    encoder = OneHotEncoder(inputCols=[str_indexer.getOutputCol()], \n",
    "                            outputCols=[column + \"Vec\"],\n",
    "                            handleInvalid='keep')\n",
    "    stages += [str_indexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler1 = VectorAssembler(inputCols=numerical_feats, \n",
    "                             outputCol=\"num_features\")\n",
    "scaler = RobustScaler(inputCol='num_features',\n",
    "                      outputCol='scaled')\n",
    "stages += [assembler1, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_inputs = [c + \"Vec\" for c in categorical_feats] + [\"scaled\"]\n",
    "assembler2 = VectorAssembler(inputCols=assembler_inputs,\n",
    "                             outputCol=\"features\")\n",
    "stages += [assembler2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(weightCol='weights')\n",
    "stages.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1.0, 0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = TrainValidationSplit(estimator=pipeline,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=BinaryClassificationEvaluator(metricName='areaUnderPR'),\n",
    "                                 trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_val.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.bestModel.stages[-1].extractParamMap())\n",
    "print(model.bestModel.stages[-1].explainParam('regParam'))\n",
    "print(model.bestModel.stages[-1].explainParam('elasticNetParam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Let's use the run-of-the-mill evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# We have only two choices: area under ROC and PR curves :-(\n",
    "auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "auprc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\",\n",
    "                                 evaluator.metricLabel: 1})\n",
    "evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\",\n",
    "                                 evaluator.metricLabel: 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
