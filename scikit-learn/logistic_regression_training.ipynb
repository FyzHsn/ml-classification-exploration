{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logisitic Regression Model Training Via sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech Spec\n",
    "* Google Cloud Compute Engine\n",
    "* n1-standard-4 (4 vCPUs, 15 GB memory)\n",
    "* Debian GNU/ Linux 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Hyperparameter tuning over the parameter grid ----- took ----- minutes.\n",
    "\n",
    "Traning over ------ samples took ----- minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime as dt\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.sparse import csc_matrix, hstack \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, \\\n",
    "                            auc, \\\n",
    "                            confusion_matrix, \\\n",
    "                            log_loss, make_scorer, \\\n",
    "                            roc_auc_score, roc_curve, \\\n",
    "                            precision_recall_curve, \\\n",
    "                            precision_score, \\\n",
    "                            recall_score, \\\n",
    "                            f1_score\n",
    "from sklearn.model_selection import GridSearchCV, \\\n",
    "                                    learning_curve, \\\n",
    "                                    StratifiedKFold, \\\n",
    "                                    train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open('../data/training_data.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "col_names = data.pop(0)\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "df.head()\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3738937, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset has over 3.5 million records and 17 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feat = ['startCount', 'viewCount', 'clickCount', 'installCount', 'install', 'startCount1d', 'startCount7d']\n",
    "categorical_nominal_feat = ['campaignId', 'sourceGameId', 'country', 'platform', 'softwareVersion', 'connectionType', 'deviceType']\n",
    "for feat in numerical_feat:\n",
    "    df[feat] = df[feat].astype('int')\n",
    "df['install'] = df['install'].astype('int')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is denoted by the binary variable _install_. The remaining features can be broken down into the categories:\n",
    "\n",
    "**Numerical**\n",
    "1. startCount\n",
    "2. viewCount\n",
    "3. clickCount\n",
    "4. installCount\n",
    "5. startCount1d\n",
    "6. startCount7d\n",
    "\n",
    "**Datetime**\n",
    "1. timestamp\n",
    "2. lastStart\n",
    "\n",
    "**Categorical nominal**\n",
    "1. id\n",
    "2. campaignId\n",
    "3. platform\n",
    "4. softwareVersion\n",
    "5. sourceGameId\n",
    "6. country\n",
    "7. connectionType\n",
    "8. deviceType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['install'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the class distribution of the install to no-install status is extremely imbalanced at 1:82. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df[numerical_feat]\n",
    "df_num.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the install status is not linearly correlated with any of the numerical variables. Some of the features such as _startCount_ and _viewCount_ are very strongly correlated as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [7, 4]})\n",
    "sns.distplot(\n",
    "    df['startCount'], norm_hist=False, kde=False, bins=20, hist_kws={\"range\": [1, 100], \"alpha\": 1}\n",
    ").set(xlabel='startCount', ylabel='Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the numerical features are fat-tailed power law distributions. Hence, when it comes to scaling these features, using the standard scaling will be a bad approach. While we only show the startCount in this plot, the other numerical features look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in categorical_nominal_feat:\n",
    "    print(feat)\n",
    "    print(\"==========\")\n",
    "    print(df[feat].value_counts())\n",
    "    print(\"        \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that the cardinality of the campaignId and sourceGameId features are very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Use timestamp and lastStart to create timeSinceLastStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_parser(datetime_str):\n",
    "    try:\n",
    "        date_str, time_str = datetime_str.split(\"T\")\n",
    "    except:\n",
    "        return \n",
    "    time_str=time_str[:8]\n",
    "    year, month, day = date_str.split(\"-\")\n",
    "    hour, minut, sec = time_str.split(\":\")\n",
    "    return dt.datetime(int(year), int(month), int(day), int(hour), int(minut), int(sec))\n",
    "        \n",
    "\n",
    "def time_diff_in_minutes(dt_0, dt_1):\n",
    "    return np.round((dt_1 - dt_0).total_seconds() / 60.0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.timestamp = df.timestamp.apply(datetime_parser)\n",
    "df.lastStart = df.lastStart.apply(datetime_parser)\n",
    "\n",
    "df['timeSinceLastStart'] = df.apply(lambda row: time_diff_in_minutes(row['lastStart'], row['timestamp']), axis=1).fillna(0) \n",
    "df = df.drop(['timestamp', 'lastStart', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### OHE Categorical Nominal Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = df[['startCount', 'viewCount', 'clickCount', 'installCount', 'startCount1d', 'startCount7d', 'timeSinceLastStart']].values\n",
    "X_cat = df[['campaignId', 'sourceGameId', 'country', 'platform', 'softwareVersion', 'connectionType', 'deviceType']]\n",
    "\n",
    "y = df['install']\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_cat = enc.fit_transform(X_cat)\n",
    "X = hstack((X_cat, X_num))\n",
    "\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vectors here are a combination of sparse and dense features (though the columns are overwhelmingly sparse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the optimal hyperparameters for the logistic regression model. In order to do that the training dataset is further split into training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "X_train = csc_matrix(X_train)\n",
    "y_train = y_train.values\n",
    "X_test = csc_matrix(X_test)\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the two simple options to balance the datset is undersampling which would involve matching the number of non-install classes to be the same as the number of install classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_fit(x, y):\n",
    "    zero_indices = []\n",
    "    one_indices = []\n",
    "    for idx, class_label in enumerate(y):\n",
    "        if class_label == 0:\n",
    "            zero_indices.append(idx)\n",
    "        else:\n",
    "            one_indices.append(idx)\n",
    "    \n",
    "    resampled_indices = one_indices + random.sample(zero_indices, len(one_indices))\n",
    "    random.shuffle(resampled_indices)\n",
    "    resampled_x, resampled_y = [], []\n",
    "    for idx in resampled_indices:\n",
    "        resampled_x.append(x[idx].toarray()[0])\n",
    "        resampled_y.append(y[idx])\n",
    "    return csc_matrix(resampled_x), resampled_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics used to measure the classifier performance other than AUROC, log-loss and prediction bias are the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_score(clf, x, y):\n",
    "    return log_loss(y, clf.predict_proba(x))\n",
    "\n",
    "def auroc_score(clf, x, y):\n",
    "    return roc_auc_score(y, clf.predict_proba(x)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Hyperparameter Tuning Via Stratified Cross Validated Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalanced_cross_validation_score(clf, x, y, cv, scoring):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=False)\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(x,y):\n",
    "        xfold_train_sampled, yfold_train_sampled = undersample_fit(x[train_idx], y[train_idx])\n",
    "        clf.fit(xfold_train_sampled, yfold_train_sampled)\n",
    "        \n",
    "        train_score = scoring(clf, xfold_train_sampled, yfold_train_sampled)\n",
    "        test_score  = scoring(clf, x[test_idx], y[test_idx])\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "    print(\"Train score: {} +/- {}, CV score: {} +/- {}\".format(np.round(np.mean(train_scores), 2),\n",
    "                                                               np.round(np.std(train_scores), 2),\n",
    "                                                               np.round(np.mean(test_scores), 2),\n",
    "                                                               np.round(np.std(test_scores), 2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter grid is specified by the C values and the penalty metrics below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "penalty = ['l1', 'l2']\n",
    "for c in C:\n",
    "    for pen in penalty:\n",
    "        print(\"C: {}, Penalty: {}\".format(str(c), pen))\n",
    "        print(\"====================\")\n",
    "        t_0 = time.time()\n",
    "        pipe_lr = Pipeline([('scl', RobustScaler(with_centering=False)),\n",
    "                           ('clf', LogisticRegression(C=c, penalty=pen, solver='liblinear', random_state=0))])\n",
    "        imbalanced_cross_validation_score(pipe_lr, X_train, y_train, 3, auroc_score)\n",
    "        print(\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stratified cross validation schema along with undersampling the non-install class to balance the dataset, we find that C=0.1 and penalty='l1' gives the optimal log-loss score of 0.63 +/- 0 on the test set and 0.62 +/- 0 on the training set. Meanwhile, for the AUROC we find a test score of 0.72 +/- 0.01 and train score of 0.73 +/- 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Learning Curve For Optimal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('scl', RobustScaler(with_centering=False)),\n",
    "                    ('clf', LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=0))])\n",
    "imbalanced_cross_validation_score(pipe_lr, X_train, y_train, 3, log_loss_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the preceding code using various sample sizes and compared their performances.\n",
    "* Sample size: 100,000, CV log-loss score - train: 0.62, test: 0.64\n",
    "* Sample size: 200,000, CV log-loss score - train: 0.61, test: 0.63\n",
    "* Sample size: 400,000, CV log-loss score - train: 0.61, test: 0.62\n",
    "* Sample size: 800,000, CV log-loss score - train: 0.62, test: 0.62\n",
    "* Sample size: 160,0000, CV log-loss score - train: 0.63, cv: 0.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, it seems that using smaller subsample of the training data is a viable option to get around memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Other Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = undersample_fit(X_train, y_train)\n",
    "pipe_lr = Pipeline([('scl', RobustScaler(with_centering=False)),\n",
    "                    ('clf', LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=0))])\n",
    "pipe_lr.fit(x, y)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print(\"Precision: {}%\".format(int(100 * precision_score(y_test, y_pred))))\n",
    "print(\"Recall: {}%\".format(int(100 * recall_score(y_test, y_pred))))\n",
    "print(\"Log-loss: {}%\".format(int(100 * log_loss_score(pipe_lr, X_test, y_test))))\n",
    "print(\"AUROC: {}%\".format(int(100 * roc_auc_score(y_test, pipe_lr.predict_proba(X_test)[:, 1]))))\n",
    "tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "print(\"True Negatives: {}, Fale Positives: {}, False Negatives: {}, True Positives: {}\".format(tn, fp, fn, tp))\n",
    "print(\"Prediction bias: {}\".format(sum(y_pred) / len(y_pred) - sum(y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the model has a very low precision of 4% but perhaps this is jutified by the recall of 72%. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
